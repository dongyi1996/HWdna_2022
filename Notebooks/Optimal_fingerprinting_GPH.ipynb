{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate 3 year non-overlapped tmax anomlies seires and calculate scaling-factor through Optimal fingerprinting and calculate the relative contributions of different external forcings using method by Ribes et al. (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Codes for detection and attribution analysis below are adopted from https://github.com/rafaelcabreu/attribution, by Rafael Abreu\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1 preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess:\n",
    "    \"\"\"\n",
    "    A class to preprocess model data to be used in attribution methods like OLS, TLS and others. The script is\n",
    "    heavily based on Aurelien Ribes (CNRM-GAME) scilab code, so for more information the user should consult the\n",
    "    following reference:\n",
    "        Ribes A., S. Planton, L. Terray (2012) Regularised optimal fingerprint for attribution.\n",
    "        Part I: method, properties and idealised analysis. Climate Dynamics.\n",
    "\n",
    "    :attribute y: numpy.ndarray\n",
    "        Array of size nt with observations as a timeseries\n",
    "    :attribute X: numpy.ndarray\n",
    "        Array of size nt x nf, where nf is the number of forcings, with the model output as a timeseries\n",
    "    :attribute Z: numpy.ndarray\n",
    "        Array of ensembles with internal variability used to compute covariance matrix\n",
    "    :attribute nt: int\n",
    "        Number of timesteps for the timeseries\n",
    "\n",
    "    :method extract_Z2(self, method='regular', frac=0.5):\n",
    "        Split big sample Z in Z1 and Z2\n",
    "    :method proj_fullrank(self, Z1, Z2):\n",
    "        Provides a projection matrix to ensure its covariance matrix to be full-ranked\n",
    "    :method creg(self, X, method='ledoit', alpha1=1e-10, alpha2=1):\n",
    "        Computes regularized covariance matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, y, X, Z):\n",
    "        \"\"\"\n",
    "        :param y: numpy.ndarray\n",
    "            Array of size nt with observations as a timeseries\n",
    "        :param X: numpy.ndarray\n",
    "            Array of size nt x nf, where nf is the number of forcings, with the model output as a timeseries\n",
    "        :param Z: numpy.ndarray\n",
    "            Array of ensembles with internal variability used to compute covariance matrix\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "        self.Z = Z\n",
    "\n",
    "        self.nt = y.shape[0]\n",
    "\n",
    "    #################################################################\n",
    "\n",
    "    def extract_Z2(self, method='regular', frac=0.5):\n",
    "        \"\"\"\n",
    "        This function is used to split a big sample Z (dimension: nz x p, containing nz iid realisation of a random\n",
    "        vector of size p) into two samples Z1 and Z2 (respectively of dimension nz1 x p and nz2 x p, with\n",
    "        nz = nz1 + nz2). Further explanations in Ribes et al. (2012).\n",
    "        :param method: str\n",
    "            type of sampling used, for now may be only 'regular'\n",
    "        :param frac: float\n",
    "            fraction of realizations to put in Z2, the remaining is used in Z1\n",
    "        :return:\n",
    "        Z1: numpy.ndarray\n",
    "            Array of size (nz1 x p)\n",
    "        Z2: numpy.ndarray\n",
    "            Array of size (nz2 x p)\n",
    "        \"\"\"\n",
    "        nz = self.Z.shape[1]\n",
    "        ind_z2 = np.zeros(nz)\n",
    "        \n",
    "        if method == 'regular':\n",
    "            # if frac = 0.5 ix would be [1, 3, 5, ..., ], so gets the index\n",
    "            # for every two points\n",
    "            ix = np.arange(1 / frac - 1, nz, 1 / frac).astype(int)  # -1 is because python starts index at 0\n",
    "            ind_z2[ix] = 1\n",
    "            Z2 = self.Z[:, ind_z2 == 1]\n",
    "            Z1 = self.Z[:, ind_z2 == 0]\n",
    "        else:\n",
    "            raise NotImplementedError('Method not implemented yet')\n",
    "\n",
    "        return Z1, Z2\n",
    "\n",
    "    #################################################################\n",
    "\n",
    "    def proj_fullrank(self, Z1, Z2):\n",
    "        \"\"\"\n",
    "        This function provides a projection matrix U that can be applied to y, X, Z1 and Z2 to ensure its covariance\n",
    "        matrix to be full-ranked. Uses variables defined in 'self', Z1 and Z2 computed in 'extract_Z2' method.\n",
    "        :param Z1: numpy.ndarray\n",
    "            Array of size (nz1 x p) of control simulation\n",
    "        :param Z2: numpy.ndarray\n",
    "            Array of size (nz1 x p) of control simulation\n",
    "        :return:\n",
    "        yc: numpy.ndarray\n",
    "            y projected in U\n",
    "        Xc: numpy.ndarray\n",
    "            X projected in U\n",
    "        Z1: numpy.ndarray\n",
    "            Z1 projected in U\n",
    "        Z2c: numpy.ndarray\n",
    "            Z2 projected in U\n",
    "        \"\"\"\n",
    "        # M: the matrix corresponding to the temporal centering\n",
    "        M = np.eye(self.nt, self.nt) - np.ones((self.nt, self.nt)) / self.nt \n",
    "\n",
    "        # Eigen-vectors/-values of M; note that rk(M)=nt-1, so M has one eigenvalue equal to 0.\n",
    "        u, d = speco(M)\n",
    "        \n",
    "        # (nt-1) first eigenvectors (ie the ones corresponding to non-zero eigenvalues)\n",
    "        U = u[:, :self.nt-1].T\n",
    "\n",
    "        # Project all input data\n",
    "        yc = np.dot(U, self.y)\n",
    "        Xc = np.dot(U, self.X)\n",
    "        Z1c = np.dot(U, Z1)\n",
    "        Z2c = np.dot(U, Z2)\n",
    "\n",
    "        return yc, Xc, Z1c, Z2c\n",
    "\n",
    "    #################################################################\n",
    "\n",
    "    def creg(self, X, method='ledoit', alpha1=1e-10, alpha2=1):\n",
    "        \"\"\"\n",
    "        This function compute the regularised covariance matrix estimate following the equation\n",
    "        'Cr = alpha1 * Ip + alpha2 * CE' where alpha1 and alpha2 are parameters Ip is the p x p identity matrix and CE\n",
    "        is the sample covariance matrix\n",
    "        :param X: numpy.ndarray\n",
    "            A n x p sample, meaning n iid realization of a random vector of size p.\n",
    "        :param method: str\n",
    "            method to compute the regularized covariance matrix\n",
    "            - 'ledoit' uses Ledoit and Wolf (2003) estimate (default)\n",
    "            - 'specified' uses specified values of alpha1 and alpha2\n",
    "        :param alpha1: float\n",
    "            Specified value for alpha1 (not used if method different than 'specified')\n",
    "        :param alpha2: float\n",
    "            Specified value for alpha1 (not used if method different than 'specified')\n",
    "        :return:\n",
    "        Cr: numpy.ndarray\n",
    "            Regularized covariance matrix\n",
    "        \"\"\"\n",
    "        n, p = X.shape\n",
    "\n",
    "        CE = np.dot(X.T, X) / n # sample covariance\n",
    "        Ip = np.eye(p, p)\n",
    "\n",
    "        # method for the regularised covariance matrix estimate as introduced by Ledoit & Wolf (2004) more specifically\n",
    "        # on pages 379-380\n",
    "        if method == 'ledoit':\n",
    "            m = np.trace(np.dot(CE, Ip)) / p  # first estimate in L&W\n",
    "            XP = CE - m * Ip\n",
    "            d2 = np.trace(np.dot(XP, XP.T)) / p  # second estimate in L&W\n",
    "\n",
    "            bt = np.zeros(n)\n",
    "\n",
    "            for i in range(n):\n",
    "                Xi = X[i, :].reshape((1, p))\n",
    "                Mi = np.dot(Xi.T, Xi) \n",
    "                bt[i] = np.trace(np.dot((Mi - CE), (Mi - CE).T)) / p\n",
    "            \n",
    "            bb2 = (1. / n ** 2.) * bt.sum()    \n",
    "            b2 = min(bb2, d2)  # third estimate in L&W\n",
    "            a2 = d2 - b2  # fourth estimate in L&W\n",
    "\n",
    "            alpha1 = (b2 * m / d2)\n",
    "            alpha2 = (a2 / d2)\n",
    "\n",
    "        elif method != 'specified':\n",
    "            raise NotImplementedError('Method not implemented yet')\n",
    "\n",
    "        Cr = alpha1 * Ip + alpha2 * CE\n",
    "\n",
    "        return Cr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2 utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speco(C):\n",
    "    \"\"\"\n",
    "    This function computes eigenvalues and eigenvectors, in descending order\n",
    "    :param C: numpy.ndarray\n",
    "        A p x p symetric real matrix\n",
    "    :return:\n",
    "    P: numpy.ndarray\n",
    "        The eigenvectors (P[:, i] is the ist eigenvector)\n",
    "    D: numpy.ndarray\n",
    "        The eigenvalues as a diagonal matrix\n",
    "    \"\"\"\n",
    "    # Compute eigenvalues and eigenvectors (the eigenvectors are non unique so the values may change from one software\n",
    "    # to another e.g. python, matlab, scilab)\n",
    "    D0, P0 = np.linalg.eig(C)\n",
    "\n",
    "    # Take real part (to avoid numeric noise, eg small complex numbers)\n",
    "    if np.max(np.imag(D0)) / np.max(np.real(D0)) > 1e-12:\n",
    "        raise ValueError(\"Matrix is not symmetric\")   \n",
    "\n",
    "    # Check that C is symetric (<=> real eigen-values/-vectors)\n",
    "    P1 = np.real(P0)\n",
    "    D1 = np.real(D0)\n",
    "\n",
    "    # sort eigenvalues in descending order and\n",
    "    # get their indices to order the eigenvector\n",
    "    Do = np.sort(D1)[::-1]\n",
    "    o = np.argsort(D1)[::-1]\n",
    "\n",
    "    P = P1[:, o]\n",
    "    D = np.diag(Do)\n",
    "\n",
    "    return P, D\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def chi2_test(d_cons, df):\n",
    "    \"\"\"\n",
    "    Check whether it is from a chi-squared distribution or not\n",
    "    :param d_cons: float\n",
    "        -2 log-likelihood\n",
    "    :param df: int\n",
    "        Degrees of freedom\n",
    "    :return:\n",
    "    pv_cons: float\n",
    "        p-value for the test\n",
    "    \"\"\"\n",
    "    rien = stats.chi2.cdf(d_cons, df=df)\n",
    "    pv_cons = 1. - rien\n",
    "\n",
    "    return pv_cons\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def project_vectors(nt, X):\n",
    "    \"\"\"\n",
    "    This function provides a projection matrix U that can be applied to X to ensure its covariance matrix to be\n",
    "    full-ranked. Projects to a nt-1 subspace (ref: Ribes et al., 2013).\n",
    "    :param nt: int\n",
    "        number of time steps\n",
    "    :param X: numpy.ndarray\n",
    "        nt x nf array to be projected\n",
    "    :return:\n",
    "    np.dot(U, X): numpy.ndarray\n",
    "        nt - 1 x nf array of projected timeseries\n",
    "    \"\"\"\n",
    "    M = np.eye(nt, nt) - np.ones((nt, nt)) / nt\n",
    "\n",
    "    # Eigen-vectors/-values of M; note that rk(M)=nt-1, so M has one eigenvalue equal to 0.\n",
    "    u, d = speco(M)\n",
    "\n",
    "    # (nt-1) first eigenvectors (ie the ones corresponding to non-zero eigenvalues)\n",
    "    U = u[:, :nt - 1].T\n",
    "\n",
    "    return np.dot(U, X)\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def unproject_vectors(nt, Xc):\n",
    "    \"\"\"\n",
    "    This function provides unprojects a matrix nt subspace to we can compute the trends\n",
    "    :param nt: int\n",
    "        number of time steps\n",
    "    :param Xc: numpy.ndarray\n",
    "        nt x nf array to be unprojected\n",
    "    :return:\n",
    "    np.dot(U, X): numpy.ndarray\n",
    "        nt - 1 x nf array of projected timeseries\n",
    "    \"\"\"\n",
    "    M = np.eye(nt, nt) - np.ones((nt, nt)) / nt\n",
    "\n",
    "    # Eigen-vectors/-values of M; note that rk(M)=nt-1, so M has one eigenvalue equal to 0.\n",
    "    u, d = speco(M)\n",
    "\n",
    "    # inverse of the projection matrix\n",
    "    Ui = np.linalg.inv(u.T)[:, :nt - 1]\n",
    "\n",
    "    return np.dot(Ui, Xc)\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def SSM(exp, X_mm, domain, init=1979, end=2009):\n",
    "    \"\"\"\n",
    "    Calculates the squared difference between each models ensemble mean and the multi-model mean. Based on\n",
    "    (Ribes et al., 2017)\n",
    "    :param exp: str\n",
    "        Experiment to calculate the difference (e.g., 'historical', 'historicalNat')\n",
    "    :param X_mm: numpy.ndarray\n",
    "        Array with multi-model ensemble mean\n",
    "    :param init: int\n",
    "        Correspondent year to start the analysis\n",
    "    :param end: int\n",
    "        Correspondent year to finish the analysis\n",
    "    :return:\n",
    "    np.diag(((Xc - Xc_mm) ** 2.).sum(axis=1)): numpy.ndarray\n",
    "        nt -1 x nt - 1 array of the difference between each model ensemble mean the multi-model mean\n",
    "    \"\"\"\n",
    "    # reads ensemble mean for each model\n",
    "    ifiles = glob(dir + 'data/model/%s/ensmean/*_%s_%s_%s.csv' % (exp, init, end, domain))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for ifile in ifiles:\n",
    "        df_temp = pd.read_csv(ifile, index_col=0, parse_dates=True)\n",
    "        df = pd.concat([df, df_temp['0'].to_frame(os.path.basename(ifile)[:-4])], axis=1)\n",
    "\n",
    "    # remove columns (ensemble members with nan)\n",
    "    df.dropna(inplace=True, axis=1)\n",
    "    # gets ensemble values and multi model (mm) ensemble\n",
    "    X = df.values\n",
    "\n",
    "    # project the data\n",
    "    Xc = project_vectors(X.shape[0], X)\n",
    "    Xc_mm = project_vectors(X.shape[0], X_mm.reshape((X.shape[0], 1)))\n",
    "\n",
    "    return np.diag(((Xc - Xc_mm) ** 2.).sum(axis=1))\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def get_nruns(exp, domain, how='pandas', init=1979, end=2009):\n",
    "    \"\"\"\n",
    "    Reads the number of runs for each model\n",
    "    :param exp: str\n",
    "        Experiment to calculate the difference (e.g., 'historical', 'historicalNat')\n",
    "    :param how: str\n",
    "        Used to see if the number of runs is calculated using the pandas dataframes or text file ('historicalOA' for\n",
    "        example)\n",
    "    :param init: int\n",
    "        Correspondent year to start the analysis\n",
    "    :param end: int\n",
    "        Correspondent year to finish the analysis\n",
    "    :return:\n",
    "    nruns: numpy.ndarray\n",
    "       Array with the number of runs for each model\n",
    "    \"\"\"\n",
    "    if how == 'pandas':\n",
    "        ifiles = glob(dir + 'data/model/%s/ensemble/*_%s_%s_%s.csv' % (exp, init, end, domain))\n",
    "        nruns = []\n",
    "\n",
    "        for ifile in sorted(ifiles):\n",
    "            df_temp = pd.read_csv(ifile, index_col=0, parse_dates=True)\n",
    "            nruns.append(len(df_temp.columns))\n",
    "\n",
    "        nruns = np.array(nruns)\n",
    "    elif how == 'loadtxt':\n",
    "        nruns = np.loadtxt('data/model/%s/ensemble/nruns_%s.txt' % (exp, exp))\n",
    "\n",
    "    return nruns\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def Cm_estimate(exp, Cv, X_mm, domain, how_nr='pandas', init=1979, end=2009):\n",
    "    \"\"\"\n",
    "    Estimated covariance matrix for model error (Ribes et al., 2017)\n",
    "    :param exp: str\n",
    "        Experiment to calculate the difference (e.g., 'historical', 'historicalNat')\n",
    "    :param Cv: numpy.ndarray\n",
    "        Array with internal variability covariance matrix\n",
    "    :param X_mm: numpy.ndarray\n",
    "        Array with multi-model ensemble mean\n",
    "    :param how_nr:\n",
    "        Used to see if the number of runs is calculated using the pandas dataframes or text file ('historicalOA' for\n",
    "        example)\n",
    "    :param init: int\n",
    "        Correspondent year to start the analysis\n",
    "    :param end: int\n",
    "        Correspondent year to finish the analysis\n",
    "    :return:\n",
    "    Cm_pos_hat: numpy.ndarray\n",
    "        Estimated covariance matrix for model error\n",
    "    \"\"\"\n",
    "\n",
    "    # model difference\n",
    "    _SSM = SSM(exp, X_mm, domain=domain, init=init, end=end)\n",
    "\n",
    "    # nruns - number of runs / nm - number of models\n",
    "    nruns = get_nruns(exp, domain=domain, how=how_nr, init=init, end=end)\n",
    "    nm = len(nruns)\n",
    "\n",
    "    Cv_all = np.zeros(Cv.shape)\n",
    "    for nr in nruns:\n",
    "        Cv_all += Cv / nr\n",
    "\n",
    "    # first estimation of Cm\n",
    "    Cm_hat = (1. / (nm - 1.)) * (_SSM - ((nm - 1.) / nm) * Cv_all)\n",
    "\n",
    "    # set negative eigenvalues to zero and recompose the signal\n",
    "    S, X = np.linalg.eig(Cm_hat)\n",
    "    S[S < 0] = 0\n",
    "    Cm_pos_hat = np.linalg.multi_dot([X, np.diag(S), np.linalg.inv(X)])  # spectral decomposition\n",
    "\n",
    "    Cm_pos_hat = (1. + (1. / nm)) * Cm_pos_hat\n",
    "\n",
    "    return Cm_pos_hat\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def Cv_estimate(exp, Cv, domain, how_nr='pandas', init=1979, end=2009):\n",
    "    \"\"\"\n",
    "    Estimated covariance matrix for internal variability considering multiple models (Ribes et al., 2017)\n",
    "    :param exp: str\n",
    "        Experiment to calculate the difference (e.g., 'historical', 'historicalNat')\n",
    "    :param Cv: numpy.ndarray\n",
    "        Array with internal variability covariance matrix\n",
    "    :param how_nr:\n",
    "        Used to see if the number of runs is calculated using the pandas dataframes or text file ('historicalOA' for\n",
    "        example)\n",
    "    :param init: int\n",
    "        Correspondent year to start the analysis\n",
    "    :param end: int\n",
    "        Correspondent year to finish the analysis\n",
    "    :return:\n",
    "    Cv_estimate: numpy.ndarray\n",
    "        Estimated covariance matrix for internal variability considering multiple models\n",
    "    \"\"\"\n",
    "    # nruns - number of runs / nm - number of models\n",
    "    nruns = get_nruns(exp, domain=domain, how=how_nr, init=init, end=end)\n",
    "    nm = len(nruns)\n",
    "\n",
    "    Cv_all = np.zeros(Cv.shape)\n",
    "    for nr in nruns:\n",
    "        Cv_all += Cv / nr\n",
    "\n",
    "    Cv_estimate = (1. / (nm ** 2.) * Cv_all)\n",
    "\n",
    "    return Cv_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3 trend calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trend(y):\n",
    "    \"\"\"\n",
    "    Calculate the trend by unprojecting a vector to the nt subspace and the using OLS estimation\n",
    "    :param y: numpy.ndarray\n",
    "        nt -1 array used to calculate the trend\n",
    "    :return:\n",
    "    beta_hat: numpy.ndarray\n",
    "        value of the scaling factor of the OLS adjustment\n",
    "    \"\"\"\n",
    "    nt = len(y) + 1\n",
    "    y_un = unproject_vectors(nt, y)  # unproject the data\n",
    "\n",
    "    X = np.vstack([np.ones(nt), np.arange(nt)]).T\n",
    "\n",
    "    # use OLS to calculate the trend\n",
    "    beta_hat = np.linalg.multi_dot([np.linalg.inv(np.linalg.multi_dot([X.T, X])), X.T, y_un])\n",
    "\n",
    "    return beta_hat[1]  # only return the trend\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def calculate_uncertainty(y, Cy, alpha=0.05, nsamples=4000):\n",
    "    \"\"\"\n",
    "    Calculate trend uncertainty by generating multiple series and the calculating the confidence interval\n",
    "    :param y: numpy.ndarray\n",
    "        nt -1 array used to calculate the trend\n",
    "    :param Cy: numpy.ndarray\n",
    "        nt -1 x nt -1 covariance matrix from the y vector\n",
    "    :param alpha: float\n",
    "        significance level\n",
    "    :param nsamples: int\n",
    "        number of repetitions\n",
    "    :return:\n",
    "    np.array([trend_min, trend_max]): np.ndarray\n",
    "        array with the minimum and maximum values from the confidence interval\n",
    "    \"\"\"\n",
    "    trends = np.zeros(nsamples)\n",
    "    for i in range(nsamples):\n",
    "        y_random = np.random.multivariate_normal(y, Cy)  # generate random vector based on the mean and cov matrix\n",
    "\n",
    "        # calculate the trend\n",
    "        trends[i] = calculate_trend(y_random)\n",
    "\n",
    "    trend_min = np.percentile(trends, (alpha * 100) / 2.)\n",
    "    trend_max = np.percentile(trends, 100 - (alpha * 100) / 2.)\n",
    "\n",
    "    return np.array([trend_min, trend_max])\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def all_trends(y_star_hat, Xi_star_hat, Cy_star_hat, Cxi_star_hat):\n",
    "    \"\"\"\n",
    "    Calculate all trends (observations and each individual forcing) and save it to a csv file\n",
    "    :param y_star_hat: np.ndarray\n",
    "        vector of observations\n",
    "    :param Xi_star_hat: np.ndarray\n",
    "        nt -1 x nf matrix of forcings where nt is the number of time steps and nf is the number of forcings\n",
    "    :param Cy_star_hat: np.ndarray\n",
    "        nt -1 x nt -1 covariance matrix for the observations\n",
    "    :param Cxi_star_hat: np.ndarray\n",
    "        nf x nt -1 x nt -1 covariance matrix for each individual forcing\n",
    "    :return:\n",
    "    df: pandas.DataFrame\n",
    "        dataframe with the trend for the observations and each of the forcings\n",
    "    \"\"\"\n",
    "    trends_list = []\n",
    "\n",
    "    trend = calculate_trend(y_star_hat)\n",
    "    confidence_interval = calculate_uncertainty(y_star_hat, Cy_star_hat, alpha=0.05)\n",
    "\n",
    "    trends_list.append(['Observation', trend, confidence_interval[0], confidence_interval[1]])\n",
    "\n",
    "    print('-' * 60)\n",
    "    print('Trends from the analysis ...')\n",
    "    print('%30s: %.3f (%.3f, %.3f)' % ('Observation', trend, confidence_interval[0], confidence_interval[1]))\n",
    "\n",
    "    nf = Xi_star_hat.shape[1]\n",
    "    for i in range(nf):\n",
    "        trend = calculate_trend(Xi_star_hat[:, i])\n",
    "        confidence_interval = calculate_uncertainty(Xi_star_hat[:, i], Cxi_star_hat[i], alpha=0.05)\n",
    "        print('%30s: %.3f (%.3f, %.3f)' % ('Forcing no %d only' % (i+1), trend, confidence_interval[0],\n",
    "                                           confidence_interval[1]))\n",
    "\n",
    "        trends_list.append(['Forcing no %d only' % (i+1), trend, confidence_interval[0], confidence_interval[1]])\n",
    "\n",
    "    # save data as csv\n",
    "    df = pd.DataFrame(trends_list, columns=['forcing', 'trend', 'trend_min', 'trend_max'])\n",
    "    # df.to_csv('trends.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "#################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4 Attribution models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributionModel:\n",
    "    \"\"\"\n",
    "    A class for attribution models. The OLS implementation is heavily based on Aurelien Ribes (CNRM-GAME) scilab code\n",
    "    (see more in 'preprocess.py'). Also, Aurelien Ribes model proposed in 2017 is implemented following the reference:\n",
    "        Ribes, Aurelien, et al. (2017) A new statistical approach to climate change detection and attribution.\n",
    "        Climate Dynamics.\n",
    "\n",
    "    :attribute X: numpy.ndarray\n",
    "        Array of size nt x nf, where nf is the number of forcings, with the model output as a timeseries\n",
    "    :attribute y: numpy.ndarray\n",
    "        Array of size nt with observations as a timeseries\n",
    "\n",
    "    :method ols(self, Cf, Proj, Z2, cons_test='AT99'):\n",
    "        Ordinary Least Square (OLS) estimation of beta from the linear model y = beta * X + epsilon\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: numpy.ndarray\n",
    "            Array of size nt x nf, where nf is the number of forcings, with the model output as a timeseries\n",
    "        :param y: numpy.ndarray\n",
    "            Array of size nt with observations as a timeseries\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "        self.nt = y.shape[0]\n",
    "        self.nr = self.nt - 1 # 1 stands for the number of spatial patterns (dealing only with timeseries)\n",
    "        self.I = X.shape[1]\n",
    "        \n",
    "    def ols(self, Cf, Proj, Z2, cons_test='AT99'):\n",
    "        \"\"\"\n",
    "        Ordinary Least Square (OLS) estimation of beta from the linear model y = beta * X + epsilon as discussed in the\n",
    "        following reference:\n",
    "            Allen, Myles R., and Simon FB Tett (1999) Checking for model consistency in optimal fingerprinting.\n",
    "            Climate Dynamics.\n",
    "        :param Cf: numpy.ndarray\n",
    "            Covariance matrix. Be sure that Cf is invertible to use this model (look at PreProcess class)\n",
    "        :param Proj: numpy.ndarray\n",
    "            Array of zeros and ones, indicating which forcings in each simulation\n",
    "        :param Z2: numpy.ndarray\n",
    "            Array of size (nz1 x p) of control simulation used to compute consistency test\n",
    "        :param cons_test: str\n",
    "            Which consistency test to be used\n",
    "            - 'AT99' the formula provided by Allen & Tett (1999) (default)\n",
    "        :return:\n",
    "        Beta_hat: dict\n",
    "            Dictionary with estimation of beta_hat and the upper and lower confidence intervals\n",
    "        \"\"\"\n",
    "\n",
    "        # computes the covariance inverse\n",
    "        Cf1 = np.linalg.inv(Cf)\n",
    "\n",
    "        _Ft = np.linalg.multi_dot([self.X.T, Cf1, self.X])\n",
    "        _Ft1 = np.linalg.inv(_Ft)\n",
    "        Ft = np.linalg.multi_dot([_Ft1, self.X.T, Cf1]).T\n",
    "\n",
    "        _y = self.y.reshape((self.nt, 1))\n",
    "        beta_hat = np.linalg.multi_dot([_y.T, Ft, Proj.T])\n",
    "\n",
    "        # 1-D confidence interval\n",
    "        nz2 = Z2.shape[1]\n",
    "        Z2t = Z2.T\n",
    "        Var_valid = np.dot(Z2t.T, Z2t) / nz2\n",
    "        Var_beta_hat = np.linalg.multi_dot([Proj, Ft.T, Var_valid, Ft, Proj.T])\n",
    "\n",
    "        beta_hat_inf = beta_hat - 2. * stats.t.cdf(0.90, df=nz2) * np.sqrt(np.diag(Var_beta_hat).T)\n",
    "        beta_hat_sup = beta_hat + 2. * stats.t.cdf(0.90, df=nz2) * np.sqrt(np.diag(Var_beta_hat).T)\n",
    "\n",
    "        # consistency check\n",
    "        epsilon = _y - np.linalg.multi_dot([self.X, np.linalg.inv(Proj), beta_hat.T])\n",
    "\n",
    "        if cons_test == 'AT99': # formula provided by Allen & Tett (1999)\n",
    "            d_cons = np.linalg.multi_dot([epsilon.T, np.linalg.pinv(Var_valid), epsilon]) / (self.nr - self.I)\n",
    "            rien = stats.f.cdf(d_cons, dfn=self.nr-self.I, dfd=nz2)\n",
    "            pv_cons = 1 - rien\n",
    "\n",
    "        # print(\"Consistency test: %s p-value: %.5f\" % (cons_test, pv_cons))\n",
    "\n",
    "        Beta_hat = {'beta_hat': beta_hat[0], 'beta_hat_inf': beta_hat_inf[0], 'beta_hat_sup': beta_hat_sup[0]}\n",
    "        \n",
    "        return Beta_hat\n",
    "\n",
    "    #################################################################\n",
    "\n",
    "    def ribes(self, Cxi, Cy):\n",
    "        \"\"\"\n",
    "        Aurelien Ribes model proposed in 2017 is implemented following the reference:\n",
    "        Ribes, Aurelien, et al. (2017) A new statistical approach to climate change detection and attribution.\n",
    "        Climate Dynamics. It considers the following set of equations:\n",
    "\n",
    "            Y_star = sum(X_star_i) for i from 1 to nf where nf is the number of forcings\n",
    "            Y = Y_star + epsilon_y\n",
    "            Xi = X_star_i + epsilon_xi\n",
    "\n",
    "        Where epislon_y ~ N(0, Cy) and epislon_xi ~ N(0, Cxi)\n",
    "\n",
    "        :param Cxi: numpy.ndarray\n",
    "            Covariance matrix for each of the forcings Xi. Should be a 3D array (nt, nt, nf)\n",
    "        :param Cy: numpy.ndarray\n",
    "            Covariance matrix for the observations.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        X = self.X.sum(axis=1)\n",
    "        Cx = Cxi.sum(axis=0)\n",
    "\n",
    "        # Estimate the true state of variables (y) and (Xi) y_star and X_star_i using the MLE y_star_hat and\n",
    "        # Xi_star_hat, respectively\n",
    "        Xi_star_hat = np.zeros(self.X.shape)\n",
    "        y_star_hat = self.y + np.linalg.multi_dot([Cy, np.linalg.inv(Cy + Cx), (X - self.y)])\n",
    "        for i in range(Xi_star_hat.shape[1]):\n",
    "            Xi_star_hat[:, i] = self.X[:, i] + np.linalg.multi_dot([Cxi[i], np.linalg.inv(Cy + Cx), (self.y - X)])\n",
    "\n",
    "        # calculates variance for Y_star_hat\n",
    "        Cy_star_hat = np.linalg.inv(np.linalg.inv(Cy) + np.linalg.inv(Cx))\n",
    "\n",
    "        # calculates variance for Xi_star_hat\n",
    "        Cxi_star_hat = np.zeros(Cxi.shape)\n",
    "        for i in range(Cxi_star_hat.shape[0]):\n",
    "            Cxi_temp = Cxi * 1.\n",
    "            # sum for every j different than i\n",
    "            Cxi_temp[i] = 0.\n",
    "            Cxi_sum = Cxi_temp.sum(axis=0)\n",
    "\n",
    "            Cxi_star_hat[i] = np.linalg.inv(np.linalg.inv(Cxi[i]) + np.linalg.inv(Cy + Cxi_sum))\n",
    "\n",
    "        # hypothesis test: compare with chi-square distribution\n",
    "        print('#' * 60)\n",
    "        print('Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...')\n",
    "\n",
    "        # (internal variability only)\n",
    "        d_cons = np.linalg.multi_dot([self.y.T, np.linalg.inv(Cy), self.y])\n",
    "        print('%30s: %.7f (%.7f)' % ('Internal variability only', chi2_test(d_cons, self.nt), np.exp(d_cons / -2.)))\n",
    "\n",
    "        # (all forcings)\n",
    "        d_cons = np.linalg.multi_dot([(self.y - X).T, np.linalg.inv(Cy + Cx), (self.y - X)])\n",
    "        print('%30s: %.7f (%.7f)' % ('All forcings', chi2_test(d_cons, self.nt), np.exp(d_cons / -2.)))\n",
    "\n",
    "        # (individual forcings)\n",
    "        for i in range(self.X.shape[1]):\n",
    "            d_cons = np.linalg.multi_dot([(self.y - self.X[:, i]).T, np.linalg.inv(Cy + Cxi[i]), (self.y - self.X[:, i])])\n",
    "            print('%30s: %.7f (%.7f)' % ('Forcing no %d only' % (i+1), chi2_test(d_cons, self.nt), np.exp(d_cons / -2.)))\n",
    "\n",
    "        return y_star_hat, Xi_star_hat, Cy_star_hat, Cxi_star_hat\n",
    "\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomalies(forcing,domain,data_run_name=None): # forcing must be one of ['reanalyses','historical','hist-GHG','hist-aer','hist-nat']\n",
    "    dir = '/Users/zeqinhuang/Documents/paper/HWdna/procData/'\n",
    "    path = dir + 'GPH_' + forcing + '_yearly_all_patterns_' + domain + '.csv'\n",
    "    hot_occur_df = pd.read_csv(path,index_col=0)\n",
    "    hot_occur_df = hot_occur_df - hot_occur_df.mean(axis=0)\n",
    "    trunc = [1] * 5 + [2] * 5 + [3] * 5 + [4] * 5 + [5] * 5 + [6] * 5 + [7] * 6\n",
    "    hot_occur_df = hot_occur_df.groupby(trunc).mean()\n",
    "    if data_run_name == None:\n",
    "        pass\n",
    "    else:\n",
    "        hot_occur_df = hot_occur_df[data_run_name]\n",
    "    return hot_occur_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piCtrl_trunc(domain):\n",
    "    dir = '/Users/zeqinhuang/Documents/paper/HWdna/procData/'\n",
    "    path = dir + 'GPH_' + 'piControl' + '_yearly_all_patterns_' + domain + '.csv'\n",
    "    hot_occur_df = pd.read_csv(path,index_col=0)\n",
    "    hot_occur_CanESM5 = hot_occur_df['CanESM5_r1i1p1f1'].dropna()\n",
    "    hot_occur_HadGEM3 = hot_occur_df['HadGEM3-GC31-LL_r1i1p1f1'].dropna()\n",
    "    hot_occur_MIROC6 = hot_occur_df['MIROC6_r1i1p1f1'].dropna()\n",
    "    hot_occur_IPSL = hot_occur_df['IPSL-CM6A-LR_r1i2p1f1'].dropna()\n",
    "    hot_occur_MRIESM = hot_occur_df['MRI-ESM2-0_r1i1p1f1'].dropna()\n",
    "    trunc_CanESM5 = int(len(hot_occur_CanESM5) / 35)\n",
    "    trunc_HadGEM3 = int(len(hot_occur_HadGEM3) / 35)\n",
    "    trunc_MIROC6 = int(len(hot_occur_MIROC6) / 35)\n",
    "    trunc_IPSL = int(len(hot_occur_IPSL) / 35)\n",
    "    trunc_MRIESM = int(len(hot_occur_MRIESM) / 35)\n",
    "    piCtrl_occur_df = pd.DataFrame()\n",
    "    trunc = [1] * 5 + [2] * 5 + [3] * 5 + [4] * 5 + [5] * 5 + [6] * 5 + [7] * 5\n",
    "    for i in range(trunc_CanESM5):\n",
    "        hot_occur_series = hot_occur_CanESM5[35 * i : 35 * i + 35]\n",
    "        hot_occur_series = hot_occur_series - hot_occur_series.mean(axis=0)\n",
    "        piCtrl_occur_df['CanESM5_r1i1p1f1_' + str(i)] = hot_occur_series.groupby(trunc).mean()\n",
    "    for i in range(trunc_HadGEM3):\n",
    "        hot_occur_series = hot_occur_HadGEM3[35 * i : 35 * i + 35]\n",
    "        hot_occur_series = hot_occur_series - hot_occur_series.mean(axis=0)\n",
    "        piCtrl_occur_df['HadGEM3-GC31-LL_r1i1p1f3_' + str(i)] = hot_occur_series.groupby(trunc).mean()\n",
    "    for i in range(trunc_MIROC6):\n",
    "        hot_occur_series = hot_occur_MIROC6[35 * i : 35 * i + 35]\n",
    "        hot_occur_series = hot_occur_series - hot_occur_series.mean(axis=0)\n",
    "        piCtrl_occur_df['MIROC6_r1i1p1f1_' + str(i)] = hot_occur_series.groupby(trunc).mean()\n",
    "    for i in range(trunc_IPSL):\n",
    "        hot_occur_series = hot_occur_IPSL[35 * i : 35 * i + 35]\n",
    "        hot_occur_series = hot_occur_series - hot_occur_series.mean(axis=0)\n",
    "        piCtrl_occur_df['IPSL-CM6A-LR_r1i1p1f1_' + str(i)] = hot_occur_series.groupby(trunc).mean()\n",
    "    for i in range(trunc_MRIESM):\n",
    "        hot_occur_series = hot_occur_MRIESM[35 * i : 35 * i + 35]\n",
    "        hot_occur_series = hot_occur_series - hot_occur_series.mean(axis=0)\n",
    "        piCtrl_occur_df['MRI-ESM2-0_r1i1p1f1_' + str(i)] = hot_occur_series.groupby(trunc).mean()\n",
    "    return piCtrl_occur_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/Users/zeqinhuang/Documents/paper/HWdna/scripts/attribution_Ribes/'\n",
    "# get_piCtrl_trunc('EAS').to_csv(dir + 'data/model/piControl/' + 'hot_extreme_occur_' + 'piControl' + '_anomalies_' + 'EAS' + '.csv')\n",
    "# get_piCtrl_trunc('EU').to_csv(dir + 'data/model/piControl/' +'hot_extreme_occur_' + 'piControl' + '_anomalies_' + 'EU' + '.csv')\n",
    "# get_piCtrl_trunc('WNA').to_csv(dir + 'data/model/piControl/' +'hot_extreme_occur_' + 'piControl' + '_anomalies_' + 'WNA' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['historical','hist-GHG','hist-aer','hist-nat']:\n",
    "    for domain in ['EU','EAS','WNA']:\n",
    "        forcing_dir = {'historical':'historical','hist-GHG':'historicalGHG','hist-nat':'historicalNat','hist-aer':'historicalOA'}\n",
    "        df_hist_ano = get_anomalies(forcing=f,domain=domain)\n",
    "        dr = df_hist_ano.columns\n",
    "        df_hist_ano_CanESM = df_hist_ano[dr[:10]]\n",
    "        df_hist_ano_CanESM.to_csv(dir + 'data/model/' + forcing_dir[f] + '/ensemble/' + 'CanESM5_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')\n",
    "        df_hist_ano_HadGEM3 = df_hist_ano[dr[10:14]]\n",
    "        df_hist_ano_HadGEM3.to_csv(dir + 'data/model/' + forcing_dir[f] + '/ensemble/' + 'HadGEM3-GC31-LL_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')\n",
    "        df_hist_ano_MIROC6 = df_hist_ano[dr[14:17]]\n",
    "        df_hist_ano_MIROC6.to_csv(dir + 'data/model/' + forcing_dir[f] + '/ensemble/' + 'MIROC6_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')\n",
    "        df_hist_ano_IPSL = df_hist_ano[dr[17:23]]\n",
    "        df_hist_ano_IPSL.to_csv(dir + 'data/model/' + forcing_dir[f] + '/ensemble/' + 'IPSL-CM6A-LR_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')\n",
    "        df_hist_ano_MRIESM = df_hist_ano[dr[23:28]]\n",
    "        df_hist_ano_MRIESM.to_csv(dir + 'data/model/' + forcing_dir[f] + '/ensemble/' + 'MRI-ESM2-0_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')\n",
    "\n",
    "        df_hist_ano_CanESM_mean = df_hist_ano_CanESM.mean(axis=1)\n",
    "        df_hist_ano_CanESM_mean.to_csv(dir + 'data/model/' + forcing_dir[f] + '/ensmean/' + 'CanESM5_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')\n",
    "        df_hist_ano_HadGEM3_mean = df_hist_ano_HadGEM3.mean(axis=1)\n",
    "        df_hist_ano_HadGEM3_mean.to_csv(dir + 'data/model/' + forcing_dir[f] + '/ensmean/' + 'HadGEM3-GC31-LL_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')\n",
    "        df_hist_ano_MIROC6_mean = df_hist_ano_MIROC6.mean(axis=1)\n",
    "        df_hist_ano_MIROC6_mean.to_csv(dir + 'data/model/' + forcing_dir[f] + '/ensmean/' + 'MIROC6_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')\n",
    "        df_hist_ano_IPSL_mean = df_hist_ano_IPSL.mean(axis=1)\n",
    "        df_hist_ano_IPSL_mean.to_csv(dir + 'data/model/' + forcing_dir[f] + '/ensmean/' + 'IPSL-CM6A-LR_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')\n",
    "        df_hist_ano_MRIESM_mean = df_hist_ano_MRIESM.mean(axis=1)\n",
    "        df_hist_ano_MRIESM_mean.to_csv(dir + 'data/model/' + forcing_dir[f] + '/ensmean/' + 'MRI-ESM2-0_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')\n",
    "\n",
    "        df_hist_ano_mm = df_hist_ano.mean(axis=1)\n",
    "        df_hist_ano_mm.to_csv(dir + 'data/model/' + forcing_dir[f] + '/multi_model/' + 'mm_'+forcing_dir[f]+'_1979_2009_' + domain +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in ['EU','EAS','WNA']:\n",
    "    df_hist_ano = get_anomalies(forcing='reanalyses',domain=domain)\n",
    "    df_hist_ano_era5 = df_hist_ano['era5']\n",
    "    df_hist_ano_ncep2 = df_hist_ano['ncep2']\n",
    "    df_hist_ano_jra55 = df_hist_ano['jra55']\n",
    "    df_hist_ano_res = df_hist_ano.mean(axis=1)\n",
    "    df_hist_ano_res.to_csv(dir + 'data/obs/' + 'reanalyses' + '_1979_2009_' + domain +'.csv')\n",
    "    df_hist_ano_era5.to_csv(dir + 'data/obs/' + 'era5' + '_1979_2009_' + domain +'.csv')\n",
    "    df_hist_ano_ncep2.to_csv(dir + 'data/obs/' + 'ncep2' + '_1979_2009_' + domain +'.csv')\n",
    "    df_hist_ano_jra55.to_csv(dir + 'data/obs/' + 'jra55' + '_1979_2009_' + domain +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/Users/zeqinhuang/Documents/paper/HWdna/scripts/attribution_Ribes/'\n",
    "# get_piCtrl_trunc('EAS').to_csv(dir + 'hot_extreme_occur_' + 'piControl' + '_anomalies_' + 'EAS' + '.csv')\n",
    "# get_piCtrl_trunc('EU').to_csv(dir + 'hot_extreme_occur_' + 'piControl' + '_anomalies_' + 'EU' + '.csv')\n",
    "# get_piCtrl_trunc('WNA').to_csv(dir + 'hot_extreme_occur_' + 'piControl' + '_anomalies_' + 'WNA' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(y, X, Z, domain, init=1979, end=2009):\n",
    "    \"\"\"\n",
    "    Main method for using Ribes et al. (2017) algorithm including observational and model error\n",
    "    :param y: numpy.ndarray\n",
    "        Vector with nt observations\n",
    "    :param X: numpy.ndarray\n",
    "        nt x nf array with model data where nf is the number of forcings\n",
    "    :param Z: numpy.ndarray\n",
    "        nt x nz array of pseudo-observations used to calculate internal variability covariance matrix\n",
    "    :param uncorr: numpy.ndarray\n",
    "        nt x nz array of ensemble of observations representing uncorrelated error\n",
    "    :param corr: numpy.ndarray\n",
    "        nt x nz array of ensemble of observations representing correlated error\n",
    "    :param init: int\n",
    "        year to start analysis\n",
    "    :param end: end\n",
    "        year to end the analysis\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # preprocess - all\n",
    "    p = PreProcess(y, X, Z)\n",
    "    Z1, Z2 = p.extract_Z2(frac=0.5)\n",
    "    yc, Xc, Z1c, Z2c = p.proj_fullrank(Z1, Z2)\n",
    "\n",
    "    # Compute covariance matrices for internal variability\n",
    "    Cv1 = p.creg(Z1c.T, method='ledoit')\n",
    "    Cv2 = p.creg(Z2c.T, method='ledoit')\n",
    "\n",
    "    # scale covariance matrix by number of ensemble members\n",
    "    nt = len(y)\n",
    "    Cx_nat = Cm_estimate('historicalNat', Cv2, X[:, 1], domain=domain,how_nr='pandas', init=init, end=end) + \\\n",
    "             Cv_estimate('historicalNat', Cv2, domain=domain, init=init, end=end, how_nr='pandas')\n",
    "    Cx_ghg = Cm_estimate('historicalGHG', Cv2, X[:, 0], domain=domain, how_nr='pandas', init=init, end=end) + \\\n",
    "             Cv_estimate('historicalGHG', Cv2, domain=domain, init=init, end=end, how_nr='pandas')\n",
    "    Cx_oa = Cm_estimate('historicalOA', Cv2, X[:, 2], domain=domain, how_nr='pandas', init=init, end=end) + \\\n",
    "            Cv_estimate('historicalOA', Cv2, domain=domain, init=init, end=end, how_nr='pandas')\n",
    "\n",
    "    Cy = Cv1\n",
    "    Cxi = np.stack([Cx_ghg,Cx_nat,Cx_oa], axis=0)\n",
    "    # starts attribution model\n",
    "    m = AttributionModel(Xc, yc)\n",
    "    y_star_hat, Xi_star_hat, Cy_star_hat, Cxi_star_hat = m.ribes(Cxi, Cy)\n",
    "\n",
    "    return y_star_hat, Xi_star_hat, Cy_star_hat, Cxi_star_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6, 6)\n",
      "############################################################\n",
      "Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...\n",
      "     Internal variability only: 0.0275769 (0.0008289)\n",
      "                  All forcings: 0.0075539 (0.0001570)\n",
      "             Forcing no 1 only: 0.1582370 (0.0096395)\n",
      "             Forcing no 2 only: 0.1105270 (0.0056459)\n",
      "             Forcing no 3 only: 0.3277633 (0.0313387)\n",
      "------------------------------------------------------------\n",
      "Trends from the analysis ...\n",
      "                   Observation: 3.401 (2.614, 4.210)\n",
      "             Forcing no 1 only: 2.489 (1.721, 3.246)\n",
      "             Forcing no 2 only: 0.286 (0.008, 0.564)\n",
      "             Forcing no 3 only: 0.626 (0.174, 1.085)\n"
     ]
    }
   ],
   "source": [
    "obs_name = 'ncep2'\n",
    "region = 'EAS'\n",
    "df_y = pd.read_csv(dir+'data/obs/'+obs_name+'_1979_2009_'+region+'.csv',index_col=0)\n",
    "y = df_y[obs_name].values\n",
    "df_hist = pd.read_csv(dir+'data/model/historical/multi_model/mm_historical_1979_2009_'+region+'.csv')['0'].values\n",
    "df_X1_ghg = pd.read_csv(dir+'data/model/historicalGHG/multi_model/mm_historicalGHG_1979_2009_'+region+'.csv')['0'].values\n",
    "df_X2_nat = pd.read_csv(dir+'data/model/historicalNAT/multi_model/mm_historicalNAT_1979_2009_'+region+'.csv')['0'].values\n",
    "df_X3_aer = pd.read_csv(dir+'data/model/historicalOA/multi_model/mm_historicalOA_1979_2009_'+region+'.csv')['0'].values\n",
    "X = np.stack([df_X1_ghg,df_X2_nat,df_X3_aer],axis=1)\n",
    "df_Z = pd.read_csv(dir + 'data/model/piControl/hot_extreme_occur_piControl_anomalies_' + region + '.csv',index_col=0)\n",
    "Z = df_Z.values\n",
    "y_star_hat, Xi_star_hat, Cy_star_hat, Cxi_star_hat = main(y, X, Z, domain=region, init=1979, end=2009)\n",
    "trends = all_trends(y_star_hat, Xi_star_hat, Cy_star_hat, Cxi_star_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_slope(var):\n",
    "    slp = linregress(range(len(var)),var).slope\n",
    "    return slp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...\n",
      "     Internal variability only: 0.0067346 (0.0001360)\n",
      "                  All forcings: 0.8910658 (0.3180344)\n",
      "             Forcing no 1 only: 0.8061115 (0.2207163)\n",
      "             Forcing no 2 only: 0.0312071 (0.0009766)\n",
      "             Forcing no 3 only: 0.1325157 (0.0073788)\n",
      "------------------------------------------------------------\n",
      "Trends from the analysis ...\n",
      "                   Observation: 4.091 (3.102, 5.077)\n",
      "             Forcing no 1 only: 3.263 (2.393, 4.103)\n",
      "             Forcing no 2 only: 0.146 (-0.503, 0.789)\n",
      "             Forcing no 3 only: 0.682 (0.178, 1.171)\n",
      "############################################################\n",
      "Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...\n",
      "     Internal variability only: 0.0007671 (0.0000097)\n",
      "                  All forcings: 0.1227680 (0.0065879)\n",
      "             Forcing no 1 only: 0.5987841 (0.1013014)\n",
      "             Forcing no 2 only: 0.0095396 (0.0002106)\n",
      "             Forcing no 3 only: 0.0646276 (0.0026279)\n",
      "------------------------------------------------------------\n",
      "Trends from the analysis ...\n",
      "                   Observation: 3.737 (2.961, 4.520)\n",
      "             Forcing no 1 only: 2.754 (1.979, 3.503)\n",
      "             Forcing no 2 only: 0.306 (0.021, 0.594)\n",
      "             Forcing no 3 only: 0.677 (0.229, 1.124)\n",
      "############################################################\n",
      "Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...\n",
      "     Internal variability only: 0.6585517 (0.1265666)\n",
      "                  All forcings: 0.0091447 (0.0001996)\n",
      "             Forcing no 1 only: 0.0804902 (0.0035775)\n",
      "             Forcing no 2 only: 0.8746156 (0.2945519)\n",
      "             Forcing no 3 only: 0.7822408 (0.2010257)\n",
      "------------------------------------------------------------\n",
      "Trends from the analysis ...\n",
      "                   Observation: 2.767 (1.892, 3.586)\n",
      "             Forcing no 1 only: 2.578 (1.774, 3.388)\n",
      "             Forcing no 2 only: 0.240 (-0.144, 0.603)\n",
      "             Forcing no 3 only: -0.051 (-0.348, 0.229)\n",
      "############################################################\n",
      "Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...\n",
      "     Internal variability only: 0.0032283 (0.0000548)\n",
      "                  All forcings: 0.9365089 (0.4050106)\n",
      "             Forcing no 1 only: 0.7945560 (0.2108890)\n",
      "             Forcing no 2 only: 0.0188194 (0.0005026)\n",
      "             Forcing no 3 only: 0.0900802 (0.0042000)\n",
      "------------------------------------------------------------\n",
      "Trends from the analysis ...\n",
      "                   Observation: 4.151 (3.182, 5.121)\n",
      "             Forcing no 1 only: 3.296 (2.455, 4.141)\n",
      "             Forcing no 2 only: 0.163 (-0.493, 0.787)\n",
      "             Forcing no 3 only: 0.693 (0.208, 1.176)\n",
      "############################################################\n",
      "Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...\n",
      "     Internal variability only: 0.0010341 (0.0000138)\n",
      "                  All forcings: 0.0455173 (0.0016223)\n",
      "             Forcing no 1 only: 0.3862022 (0.0420040)\n",
      "             Forcing no 2 only: 0.0095156 (0.0002099)\n",
      "             Forcing no 3 only: 0.0726373 (0.0030948)\n",
      "------------------------------------------------------------\n",
      "Trends from the analysis ...\n",
      "                   Observation: 3.719 (2.910, 4.504)\n",
      "             Forcing no 1 only: 2.746 (1.979, 3.492)\n",
      "             Forcing no 2 only: 0.304 (0.012, 0.594)\n",
      "             Forcing no 3 only: 0.669 (0.229, 1.116)\n",
      "############################################################\n",
      "Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...\n",
      "     Internal variability only: 0.0072006 (0.0001478)\n",
      "                  All forcings: 0.5640030 (0.0887547)\n",
      "             Forcing no 1 only: 0.9036334 (0.3382674)\n",
      "             Forcing no 2 only: 0.0902807 (0.0042134)\n",
      "             Forcing no 3 only: 0.0260068 (0.0007672)\n",
      "------------------------------------------------------------\n",
      "Trends from the analysis ...\n",
      "                   Observation: 3.318 (2.469, 4.186)\n",
      "             Forcing no 1 only: 3.005 (2.212, 3.818)\n",
      "             Forcing no 2 only: 0.310 (-0.064, 0.675)\n",
      "             Forcing no 3 only: 0.004 (-0.285, 0.293)\n",
      "############################################################\n",
      "Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...\n",
      "     Internal variability only: 0.0061411 (0.0001212)\n",
      "                  All forcings: 0.9200177 (0.3685188)\n",
      "             Forcing no 1 only: 0.7618421 (0.1859132)\n",
      "             Forcing no 2 only: 0.0312460 (0.0009783)\n",
      "             Forcing no 3 only: 0.1300467 (0.0071751)\n",
      "------------------------------------------------------------\n",
      "Trends from the analysis ...\n",
      "                   Observation: 4.049 (3.054, 4.999)\n",
      "             Forcing no 1 only: 3.236 (2.378, 4.063)\n",
      "             Forcing no 2 only: 0.138 (-0.489, 0.791)\n",
      "             Forcing no 3 only: 0.676 (0.198, 1.168)\n",
      "############################################################\n",
      "Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...\n",
      "     Internal variability only: 0.0275769 (0.0008289)\n",
      "                  All forcings: 0.0075539 (0.0001570)\n",
      "             Forcing no 1 only: 0.1582370 (0.0096395)\n",
      "             Forcing no 2 only: 0.1105270 (0.0056459)\n",
      "             Forcing no 3 only: 0.3277633 (0.0313387)\n",
      "------------------------------------------------------------\n",
      "Trends from the analysis ...\n",
      "                   Observation: 3.401 (2.595, 4.210)\n",
      "             Forcing no 1 only: 2.489 (1.736, 3.270)\n",
      "             Forcing no 2 only: 0.286 (0.006, 0.568)\n",
      "             Forcing no 3 only: 0.626 (0.166, 1.078)\n",
      "############################################################\n",
      "Hypothesis testing p-value for Chi-2 distribution and Maximum Likelihood ...\n",
      "     Internal variability only: 0.0251356 (0.0007336)\n",
      "                  All forcings: 0.4572924 (0.0577635)\n",
      "             Forcing no 1 only: 0.7538201 (0.1803461)\n",
      "             Forcing no 2 only: 0.2241110 (0.0166080)\n",
      "             Forcing no 3 only: 0.1013605 (0.0049778)\n",
      "------------------------------------------------------------\n",
      "Trends from the analysis ...\n",
      "                   Observation: 3.113 (2.240, 3.958)\n",
      "             Forcing no 1 only: 2.835 (2.043, 3.654)\n",
      "             Forcing no 2 only: 0.287 (-0.093, 0.681)\n",
      "             Forcing no 3 only: -0.008 (-0.311, 0.304)\n"
     ]
    }
   ],
   "source": [
    "def _compute_slope(var):\n",
    "    slp = linregress(range(len(var)),var).slope\n",
    "    return slp\n",
    "\n",
    "trends_all = pd.DataFrame()\n",
    "for obs_name in ['era5','jra55','ncep2']:\n",
    "    for region in ['EU','EAS','WNA']:\n",
    "        df_y = pd.read_csv(dir+'data/obs/'+obs_name+'_1979_2009_'+region+'.csv',index_col=0)\n",
    "        y = df_y[obs_name].values\n",
    "        df_hist = pd.read_csv(dir+'data/model/historical/multi_model/mm_historical_1979_2009_'+region+'.csv')['0'].values\n",
    "        df_X1_ghg = pd.read_csv(dir+'data/model/historicalGHG/multi_model/mm_historicalGHG_1979_2009_'+region+'.csv')['0'].values\n",
    "        df_X2_nat = pd.read_csv(dir+'data/model/historicalNAT/multi_model/mm_historicalNAT_1979_2009_'+region+'.csv')['0'].values\n",
    "        df_X3_aer = pd.read_csv(dir+'data/model/historicalOA/multi_model/mm_historicalOA_1979_2009_'+region+'.csv')['0'].values\n",
    "        X = np.stack([df_X1_ghg,df_X2_nat,df_X3_aer],axis=1)\n",
    "        df_Z = pd.read_csv(dir + 'data/model/piControl/hot_extreme_occur_piControl_anomalies_' + region + '.csv',index_col=0)\n",
    "        Z = df_Z.values\n",
    "        y_star_hat, Xi_star_hat, Cy_star_hat, Cxi_star_hat = main(y, X, Z, domain=region, init=1979, end=2009)\n",
    "        trends = all_trends(y_star_hat, Xi_star_hat, Cy_star_hat, Cxi_star_hat)\n",
    "        trends['obs_name'] = obs_name\n",
    "        trends['domain'] = region\n",
    "        trends['sf_best'] = np.NAN\n",
    "        trends['sf_min'] = np.NAN\n",
    "        trends['sf_max'] = np.NAN\n",
    "\n",
    "        trends.loc[0,'sf_best'] = trends['trend'].loc[0] / _compute_slope(y)\n",
    "        trends.loc[0,'sf_min'] = trends['trend_min'].loc[0] / _compute_slope(y)\n",
    "        trends.loc[0,'sf_max'] = trends['trend_max'].loc[0] / _compute_slope(y)\n",
    "        trends.loc[1,'sf_best'] = trends['trend'].loc[1] / _compute_slope(df_X1_ghg)\n",
    "        trends.loc[1,'sf_min'] = trends['trend_min'].loc[1] / _compute_slope(df_X1_ghg)\n",
    "        trends.loc[1,'sf_max'] = trends['trend_max'].loc[1] / _compute_slope(df_X1_ghg)\n",
    "        trends.loc[2,'sf_best'] = trends['trend'].loc[2] / _compute_slope(df_X2_nat)\n",
    "        trends.loc[2,'sf_min'] = trends['trend_min'].loc[2] / _compute_slope(df_X2_nat)\n",
    "        trends.loc[2,'sf_max'] = trends['trend_max'].loc[2] / _compute_slope(df_X2_nat)\n",
    "        trends.loc[3,'sf_best'] = trends['trend'].loc[3] / _compute_slope(df_X3_aer)\n",
    "        trends.loc[3,'sf_min'] = trends['trend_min'].loc[3] / _compute_slope(df_X3_aer)\n",
    "        trends.loc[3,'sf_max'] = trends['trend_max'].loc[3] / _compute_slope(df_X3_aer)\n",
    "\n",
    "        trends_all = pd.concat([trends_all,trends],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_all.to_csv(dir + 'trends_scaling_factors_Ribes_GPH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### two signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_all = pd.DataFrame()\n",
    "for obs_name in ['era5','jra55','ncep2']:\n",
    "    for region in ['EU','EAS','WNA']:\n",
    "        df_y = pd.read_csv(dir+'data/obs/'+obs_name+'_1979_2009_'+region+'.csv',index_col=0)\n",
    "        y = df_y[obs_name].values\n",
    "\n",
    "        df_hist = pd.read_csv(dir+'data/model/historical/multi_model/mm_historical_1979_2009_'+region+'.csv')['0'].values\n",
    "        df_nat = pd.read_csv(dir+'data/model/historicalNAT/multi_model/mm_historicalNAT_1979_2009_'+region+'.csv')['0'].values\n",
    "        df_ant = df_hist - df_nat\n",
    "        X = np.stack([df_ant,df_nat],axis=1)\n",
    "\n",
    "        df_Z = pd.read_csv(dir + 'data/model/piControl/hot_extreme_occur_piControl_anomalies_' + region + '.csv',index_col=0)\n",
    "        Z = df_Z.values\n",
    "\n",
    "        p = PreProcess(y, X, Z)\n",
    "        Z1, Z2 = p.extract_Z2(frac=0.45)\n",
    "        yc, Xc, Z1c, Z2c = p.proj_fullrank(Z1, Z2)\n",
    "\n",
    "        Cr = p.creg(Z1c.T, method='ledoit')\n",
    "        m = AttributionModel(Xc, yc)\n",
    "        beta_ols = m.ols(Cr, np.array([[1, 0], [0, 1]]), Z2c)\n",
    "        beta_ols = pd.DataFrame(beta_ols)\n",
    "        beta_ols.columns = ['sf_best','sf_min','sf_max']\n",
    "\n",
    "        beta_ols['trend'] = np.NAN\n",
    "        beta_ols['trend_min'] = np.NAN\n",
    "        beta_ols['trend_max'] = np.NAN\n",
    "\n",
    "        beta_ols.loc[0,'trend'] = beta_ols.loc[0,'sf_best'] * _compute_slope(df_ant)\n",
    "        beta_ols.loc[1,'trend'] = beta_ols.loc[1,'sf_best'] * _compute_slope(df_nat)\n",
    "        beta_ols.loc[2,'trend'] = beta_ols.loc[0,'trend'] + beta_ols.loc[1,'trend']\n",
    "        beta_ols.loc[0,'trend_min'] = beta_ols.loc[0,'sf_min'] * _compute_slope(df_ant)\n",
    "        beta_ols.loc[1,'trend_min'] = beta_ols.loc[1,'sf_min'] * _compute_slope(df_nat)\n",
    "        beta_ols.loc[2,'trend_min'] = beta_ols.loc[0,'trend_min'] + beta_ols.loc[1,'trend_min']\n",
    "        beta_ols.loc[0,'trend_max'] = beta_ols.loc[0,'sf_max'] * _compute_slope(df_ant)\n",
    "        beta_ols.loc[1,'trend_max'] = beta_ols.loc[1,'sf_max'] * _compute_slope(df_nat)\n",
    "        beta_ols.loc[2,'trend_max'] = beta_ols.loc[0,'trend_max'] + beta_ols.loc[1,'trend_max']\n",
    "        # beta_ols.index = ['ANT','NAT','ALL']\n",
    "        beta_ols['forcing'] = ['ANT','NAT','ALL']\n",
    "        beta_ols['obs_name'] = obs_name\n",
    "        beta_ols['domain'] = region\n",
    "\n",
    "        trends_all = pd.concat([trends_all,beta_ols],axis=0,ignore_index=True)       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sf_best</th>\n",
       "      <th>sf_min</th>\n",
       "      <th>sf_max</th>\n",
       "      <th>trend</th>\n",
       "      <th>trend_min</th>\n",
       "      <th>trend_max</th>\n",
       "      <th>forcing</th>\n",
       "      <th>obs_name</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.709514</td>\n",
       "      <td>0.316273</td>\n",
       "      <td>1.102755</td>\n",
       "      <td>3.341844</td>\n",
       "      <td>1.489662</td>\n",
       "      <td>5.194027</td>\n",
       "      <td>ANT</td>\n",
       "      <td>era5</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.131093</td>\n",
       "      <td>-1.620312</td>\n",
       "      <td>3.882497</td>\n",
       "      <td>0.255093</td>\n",
       "      <td>-0.365426</td>\n",
       "      <td>0.875613</td>\n",
       "      <td>NAT</td>\n",
       "      <td>era5</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.596938</td>\n",
       "      <td>1.124236</td>\n",
       "      <td>6.069640</td>\n",
       "      <td>ALL</td>\n",
       "      <td>era5</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.462910</td>\n",
       "      <td>0.191496</td>\n",
       "      <td>0.734324</td>\n",
       "      <td>2.228926</td>\n",
       "      <td>0.922060</td>\n",
       "      <td>3.535791</td>\n",
       "      <td>ANT</td>\n",
       "      <td>era5</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.667737</td>\n",
       "      <td>-1.128478</td>\n",
       "      <td>2.463952</td>\n",
       "      <td>0.238383</td>\n",
       "      <td>-0.402868</td>\n",
       "      <td>0.879634</td>\n",
       "      <td>NAT</td>\n",
       "      <td>era5</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.467309</td>\n",
       "      <td>0.519192</td>\n",
       "      <td>4.415425</td>\n",
       "      <td>ALL</td>\n",
       "      <td>era5</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.216988</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>0.493054</td>\n",
       "      <td>0.962925</td>\n",
       "      <td>-0.262169</td>\n",
       "      <td>2.188018</td>\n",
       "      <td>ANT</td>\n",
       "      <td>era5</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.286442</td>\n",
       "      <td>-1.490492</td>\n",
       "      <td>2.063376</td>\n",
       "      <td>0.105153</td>\n",
       "      <td>-0.547160</td>\n",
       "      <td>0.757465</td>\n",
       "      <td>NAT</td>\n",
       "      <td>era5</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.068077</td>\n",
       "      <td>-0.809328</td>\n",
       "      <td>2.945483</td>\n",
       "      <td>ALL</td>\n",
       "      <td>era5</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.745073</td>\n",
       "      <td>0.351833</td>\n",
       "      <td>1.138314</td>\n",
       "      <td>3.509331</td>\n",
       "      <td>1.657148</td>\n",
       "      <td>5.361513</td>\n",
       "      <td>ANT</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.358060</td>\n",
       "      <td>-1.393344</td>\n",
       "      <td>4.109465</td>\n",
       "      <td>0.306281</td>\n",
       "      <td>-0.314239</td>\n",
       "      <td>0.926800</td>\n",
       "      <td>NAT</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.815612</td>\n",
       "      <td>1.342910</td>\n",
       "      <td>6.288313</td>\n",
       "      <td>ALL</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.195387</td>\n",
       "      <td>0.738214</td>\n",
       "      <td>2.247658</td>\n",
       "      <td>0.940792</td>\n",
       "      <td>3.554523</td>\n",
       "      <td>ANT</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.166411</td>\n",
       "      <td>-1.629804</td>\n",
       "      <td>1.962627</td>\n",
       "      <td>0.059409</td>\n",
       "      <td>-0.581842</td>\n",
       "      <td>0.700660</td>\n",
       "      <td>NAT</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.307067</td>\n",
       "      <td>0.358950</td>\n",
       "      <td>4.255183</td>\n",
       "      <td>ALL</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.182591</td>\n",
       "      <td>0.734723</td>\n",
       "      <td>2.035377</td>\n",
       "      <td>0.810283</td>\n",
       "      <td>3.260470</td>\n",
       "      <td>ANT</td>\n",
       "      <td>jra55</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.074753</td>\n",
       "      <td>-0.702181</td>\n",
       "      <td>2.851687</td>\n",
       "      <td>0.394542</td>\n",
       "      <td>-0.257771</td>\n",
       "      <td>1.046854</td>\n",
       "      <td>NAT</td>\n",
       "      <td>jra55</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.429919</td>\n",
       "      <td>0.552513</td>\n",
       "      <td>4.307324</td>\n",
       "      <td>ALL</td>\n",
       "      <td>jra55</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.690984</td>\n",
       "      <td>0.297743</td>\n",
       "      <td>1.084225</td>\n",
       "      <td>3.254567</td>\n",
       "      <td>1.402385</td>\n",
       "      <td>5.106750</td>\n",
       "      <td>ANT</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.693814</td>\n",
       "      <td>-1.057591</td>\n",
       "      <td>4.445218</td>\n",
       "      <td>0.382003</td>\n",
       "      <td>-0.238517</td>\n",
       "      <td>1.002522</td>\n",
       "      <td>NAT</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.636570</td>\n",
       "      <td>1.163868</td>\n",
       "      <td>6.109272</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.304375</td>\n",
       "      <td>0.032961</td>\n",
       "      <td>0.575789</td>\n",
       "      <td>1.465576</td>\n",
       "      <td>0.158710</td>\n",
       "      <td>2.772441</td>\n",
       "      <td>ANT</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.412678</td>\n",
       "      <td>-1.383538</td>\n",
       "      <td>2.208893</td>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.493925</td>\n",
       "      <td>0.788577</td>\n",
       "      <td>NAT</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.612902</td>\n",
       "      <td>-0.335214</td>\n",
       "      <td>3.561018</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.381853</td>\n",
       "      <td>0.105787</td>\n",
       "      <td>0.657919</td>\n",
       "      <td>1.694542</td>\n",
       "      <td>0.469449</td>\n",
       "      <td>2.919636</td>\n",
       "      <td>ANT</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.279884</td>\n",
       "      <td>-0.497050</td>\n",
       "      <td>3.056818</td>\n",
       "      <td>0.469845</td>\n",
       "      <td>-0.182467</td>\n",
       "      <td>1.122158</td>\n",
       "      <td>NAT</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.164388</td>\n",
       "      <td>0.286982</td>\n",
       "      <td>4.041793</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sf_best    sf_min    sf_max     trend  trend_min  trend_max forcing  \\\n",
       "0   0.709514  0.316273  1.102755  3.341844   1.489662   5.194027     ANT   \n",
       "1   1.131093 -1.620312  3.882497  0.255093  -0.365426   0.875613     NAT   \n",
       "2        NaN       NaN       NaN  3.596938   1.124236   6.069640     ALL   \n",
       "3   0.462910  0.191496  0.734324  2.228926   0.922060   3.535791     ANT   \n",
       "4   0.667737 -1.128478  2.463952  0.238383  -0.402868   0.879634     NAT   \n",
       "5        NaN       NaN       NaN  2.467309   0.519192   4.415425     ALL   \n",
       "6   0.216988 -0.059078  0.493054  0.962925  -0.262169   2.188018     ANT   \n",
       "7   0.286442 -1.490492  2.063376  0.105153  -0.547160   0.757465     NAT   \n",
       "8        NaN       NaN       NaN  1.068077  -0.809328   2.945483     ALL   \n",
       "9   0.745073  0.351833  1.138314  3.509331   1.657148   5.361513     ANT   \n",
       "10  1.358060 -1.393344  4.109465  0.306281  -0.314239   0.926800     NAT   \n",
       "11       NaN       NaN       NaN  3.815612   1.342910   6.288313     ALL   \n",
       "12  0.466800  0.195387  0.738214  2.247658   0.940792   3.554523     ANT   \n",
       "13  0.166411 -1.629804  1.962627  0.059409  -0.581842   0.700660     NAT   \n",
       "14       NaN       NaN       NaN  2.307067   0.358950   4.255183     ALL   \n",
       "15  0.458657  0.182591  0.734723  2.035377   0.810283   3.260470     ANT   \n",
       "16  1.074753 -0.702181  2.851687  0.394542  -0.257771   1.046854     NAT   \n",
       "17       NaN       NaN       NaN  2.429919   0.552513   4.307324     ALL   \n",
       "18  0.690984  0.297743  1.084225  3.254567   1.402385   5.106750     ANT   \n",
       "19  1.693814 -1.057591  4.445218  0.382003  -0.238517   1.002522     NAT   \n",
       "20       NaN       NaN       NaN  3.636570   1.163868   6.109272     ALL   \n",
       "21  0.304375  0.032961  0.575789  1.465576   0.158710   2.772441     ANT   \n",
       "22  0.412678 -1.383538  2.208893  0.147326  -0.493925   0.788577     NAT   \n",
       "23       NaN       NaN       NaN  1.612902  -0.335214   3.561018     ALL   \n",
       "24  0.381853  0.105787  0.657919  1.694542   0.469449   2.919636     ANT   \n",
       "25  1.279884 -0.497050  3.056818  0.469845  -0.182467   1.122158     NAT   \n",
       "26       NaN       NaN       NaN  2.164388   0.286982   4.041793     ALL   \n",
       "\n",
       "   obs_name domain  \n",
       "0      era5     EU  \n",
       "1      era5     EU  \n",
       "2      era5     EU  \n",
       "3      era5    EAS  \n",
       "4      era5    EAS  \n",
       "5      era5    EAS  \n",
       "6      era5    WNA  \n",
       "7      era5    WNA  \n",
       "8      era5    WNA  \n",
       "9     jra55     EU  \n",
       "10    jra55     EU  \n",
       "11    jra55     EU  \n",
       "12    jra55    EAS  \n",
       "13    jra55    EAS  \n",
       "14    jra55    EAS  \n",
       "15    jra55    WNA  \n",
       "16    jra55    WNA  \n",
       "17    jra55    WNA  \n",
       "18    ncep2     EU  \n",
       "19    ncep2     EU  \n",
       "20    ncep2     EU  \n",
       "21    ncep2    EAS  \n",
       "22    ncep2    EAS  \n",
       "23    ncep2    EAS  \n",
       "24    ncep2    WNA  \n",
       "25    ncep2    WNA  \n",
       "26    ncep2    WNA  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_all.to_csv(dir + 'trends_scaling_factors_2signal_GPH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### three signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_all = pd.DataFrame()\n",
    "for obs_name in ['era5','jra55','ncep2']:\n",
    "    for region in ['EU','EAS','WNA']:\n",
    "        df_y = pd.read_csv(dir+'data/obs/'+obs_name+'_1979_2009_'+region+'.csv',index_col=0)\n",
    "        y = df_y[obs_name].values\n",
    "\n",
    "        df_hist = pd.read_csv(dir+'data/model/historical/multi_model/mm_historical_1979_2009_'+region+'.csv')['0'].values\n",
    "        df_nat = pd.read_csv(dir+'data/model/historicalNAT/multi_model/mm_historicalNAT_1979_2009_'+region+'.csv')['0'].values\n",
    "        df_ghg = pd.read_csv(dir+'data/model/historicalGHG/multi_model/mm_historicalGHG_1979_2009_'+region+'.csv')['0'].values * 0.9\n",
    "        df_aer = pd.read_csv(dir+'data/model/historicalOA/multi_model/mm_historicalOA_1979_2009_'+region+'.csv')['0'].values\n",
    "        # df_aer = df_hist - df_ghg - df_nat\n",
    "        X = np.stack([df_ghg,df_nat,df_aer],axis=1)\n",
    "        # print('###########' * 5)\n",
    "        # print(X)\n",
    "        # print('***' * 20)\n",
    "        # print(y)\n",
    "\n",
    "        df_Z = pd.read_csv(dir + 'data/model/piControl/hot_extreme_occur_piControl_anomalies_' + region + '.csv',index_col=0)\n",
    "        Z = df_Z.values\n",
    "\n",
    "        p = PreProcess(y, X, Z)\n",
    "        Z1, Z2 = p.extract_Z2(frac=0.45)\n",
    "        yc, Xc, Z1c, Z2c = p.proj_fullrank(Z1, Z2)\n",
    "\n",
    "        Cr = p.creg(Z1c.T, method='ledoit')\n",
    "        m = AttributionModel(Xc, yc)\n",
    "        beta_ols = m.ols(Cr, np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), Z2c)\n",
    "        beta_ols = pd.DataFrame(beta_ols)\n",
    "        beta_ols.columns = ['sf_best','sf_min','sf_max']\n",
    "\n",
    "        beta_ols['trend'] = np.NAN\n",
    "        beta_ols['trend_min'] = np.NAN\n",
    "        beta_ols['trend_max'] = np.NAN\n",
    "\n",
    "        beta_ols.loc[0,'trend'] = beta_ols.loc[0,'sf_best'] * _compute_slope(df_ghg)\n",
    "        beta_ols.loc[1,'trend'] = beta_ols.loc[1,'sf_best'] * _compute_slope(df_nat)\n",
    "        beta_ols.loc[2,'trend'] = beta_ols.loc[2,'sf_best'] * _compute_slope(df_aer)\n",
    "        beta_ols.loc[3,'trend'] = beta_ols.loc[0,'trend'] + beta_ols.loc[1,'trend'] + beta_ols.loc[2,'trend']\n",
    "        beta_ols.loc[0,'trend_min'] = beta_ols.loc[0,'sf_min'] * _compute_slope(df_ghg)\n",
    "        beta_ols.loc[1,'trend_min'] = beta_ols.loc[1,'sf_min'] * _compute_slope(df_nat)\n",
    "        beta_ols.loc[2,'trend_min'] = beta_ols.loc[2,'sf_min'] * _compute_slope(df_aer)\n",
    "        beta_ols.loc[3,'trend_min'] = beta_ols.loc[0,'trend_min'] + beta_ols.loc[1,'trend_min'] + beta_ols.loc[2,'trend_min']\n",
    "        beta_ols.loc[0,'trend_max'] = beta_ols.loc[0,'sf_max'] * _compute_slope(df_ghg)\n",
    "        beta_ols.loc[1,'trend_max'] = beta_ols.loc[1,'sf_max'] * _compute_slope(df_nat)\n",
    "        beta_ols.loc[2,'trend_max'] = beta_ols.loc[2,'sf_max'] * _compute_slope(df_aer)\n",
    "        beta_ols.loc[3,'trend_max'] = beta_ols.loc[0,'trend_max'] + beta_ols.loc[1,'trend_max'] + beta_ols.loc[2,'trend_max']\n",
    "\n",
    "        beta_ols.loc[3,'sf_best'] = beta_ols.loc[3,'trend'] / _compute_slope(df_y[obs_name])\n",
    "        beta_ols.loc[3,'sf_min'] = beta_ols.loc[3,'trend_min'] / _compute_slope(df_y[obs_name])\n",
    "        beta_ols.loc[3,'sf_max'] = beta_ols.loc[3,'trend_max'] / _compute_slope(df_y[obs_name])\n",
    "\n",
    "        # beta_ols['forcing'] = ['GHG','NAT','AER']\n",
    "        beta_ols['forcing'] = ['GHG','NAT','AER','ALL']\n",
    "        beta_ols['obs_name'] = obs_name\n",
    "        beta_ols['domain'] = region\n",
    "\n",
    "        trends_all = pd.concat([trends_all,beta_ols],axis=0,ignore_index=True)       \n",
    "\n",
    "# trends_all.to_csv(dir + 'trends_scaling_factors_3signal_ols_GPH.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sf_best</th>\n",
       "      <th>sf_min</th>\n",
       "      <th>sf_max</th>\n",
       "      <th>trend</th>\n",
       "      <th>trend_min</th>\n",
       "      <th>trend_max</th>\n",
       "      <th>forcing</th>\n",
       "      <th>obs_name</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.628718</td>\n",
       "      <td>0.034368</td>\n",
       "      <td>1.223067</td>\n",
       "      <td>1.936660</td>\n",
       "      <td>0.105865</td>\n",
       "      <td>3.767455</td>\n",
       "      <td>GHG</td>\n",
       "      <td>era5</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.360311</td>\n",
       "      <td>-3.845594</td>\n",
       "      <td>3.124972</td>\n",
       "      <td>-0.081260</td>\n",
       "      <td>-0.867290</td>\n",
       "      <td>0.704769</td>\n",
       "      <td>NAT</td>\n",
       "      <td>era5</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.127583</td>\n",
       "      <td>-0.393924</td>\n",
       "      <td>4.649090</td>\n",
       "      <td>1.563543</td>\n",
       "      <td>-0.289491</td>\n",
       "      <td>3.416577</td>\n",
       "      <td>AER</td>\n",
       "      <td>era5</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.962847</td>\n",
       "      <td>-0.295960</td>\n",
       "      <td>2.221655</td>\n",
       "      <td>3.418943</td>\n",
       "      <td>-1.050916</td>\n",
       "      <td>7.888801</td>\n",
       "      <td>ALL</td>\n",
       "      <td>era5</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.689954</td>\n",
       "      <td>-0.388380</td>\n",
       "      <td>1.768287</td>\n",
       "      <td>2.154775</td>\n",
       "      <td>-1.212940</td>\n",
       "      <td>5.522490</td>\n",
       "      <td>GHG</td>\n",
       "      <td>era5</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.516582</td>\n",
       "      <td>-2.044456</td>\n",
       "      <td>3.077620</td>\n",
       "      <td>0.184420</td>\n",
       "      <td>-0.729873</td>\n",
       "      <td>1.098714</td>\n",
       "      <td>NAT</td>\n",
       "      <td>era5</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.112446</td>\n",
       "      <td>-4.671872</td>\n",
       "      <td>4.896763</td>\n",
       "      <td>0.086606</td>\n",
       "      <td>-3.598308</td>\n",
       "      <td>3.771521</td>\n",
       "      <td>AER</td>\n",
       "      <td>era5</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.891933</td>\n",
       "      <td>-2.037392</td>\n",
       "      <td>3.821257</td>\n",
       "      <td>2.425802</td>\n",
       "      <td>-5.541121</td>\n",
       "      <td>10.392724</td>\n",
       "      <td>ALL</td>\n",
       "      <td>era5</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.348429</td>\n",
       "      <td>-0.191647</td>\n",
       "      <td>0.888505</td>\n",
       "      <td>1.079196</td>\n",
       "      <td>-0.593592</td>\n",
       "      <td>2.751985</td>\n",
       "      <td>GHG</td>\n",
       "      <td>era5</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.368774</td>\n",
       "      <td>-4.246845</td>\n",
       "      <td>3.509297</td>\n",
       "      <td>-0.135377</td>\n",
       "      <td>-1.559016</td>\n",
       "      <td>1.288263</td>\n",
       "      <td>NAT</td>\n",
       "      <td>era5</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.953525</td>\n",
       "      <td>-3.703179</td>\n",
       "      <td>5.610228</td>\n",
       "      <td>0.048013</td>\n",
       "      <td>-0.186466</td>\n",
       "      <td>0.282492</td>\n",
       "      <td>AER</td>\n",
       "      <td>era5</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.914144</td>\n",
       "      <td>-2.155859</td>\n",
       "      <td>3.984147</td>\n",
       "      <td>0.991833</td>\n",
       "      <td>-2.339075</td>\n",
       "      <td>4.322740</td>\n",
       "      <td>ALL</td>\n",
       "      <td>era5</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.673497</td>\n",
       "      <td>0.079147</td>\n",
       "      <td>1.267847</td>\n",
       "      <td>2.074595</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>3.905390</td>\n",
       "      <td>GHG</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.167666</td>\n",
       "      <td>-3.652949</td>\n",
       "      <td>3.317617</td>\n",
       "      <td>-0.037813</td>\n",
       "      <td>-0.823843</td>\n",
       "      <td>0.748216</td>\n",
       "      <td>NAT</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.169803</td>\n",
       "      <td>-0.351705</td>\n",
       "      <td>4.691310</td>\n",
       "      <td>1.594569</td>\n",
       "      <td>-0.258465</td>\n",
       "      <td>3.447604</td>\n",
       "      <td>AER</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.972941</td>\n",
       "      <td>-0.224660</td>\n",
       "      <td>2.170541</td>\n",
       "      <td>3.631351</td>\n",
       "      <td>-0.838508</td>\n",
       "      <td>8.101210</td>\n",
       "      <td>ALL</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.689175</td>\n",
       "      <td>-0.389159</td>\n",
       "      <td>1.767508</td>\n",
       "      <td>2.152342</td>\n",
       "      <td>-1.215373</td>\n",
       "      <td>5.520057</td>\n",
       "      <td>GHG</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.046726</td>\n",
       "      <td>-2.514312</td>\n",
       "      <td>2.607765</td>\n",
       "      <td>0.016681</td>\n",
       "      <td>-0.897612</td>\n",
       "      <td>0.930975</td>\n",
       "      <td>NAT</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.108393</td>\n",
       "      <td>-4.675925</td>\n",
       "      <td>4.892710</td>\n",
       "      <td>0.083485</td>\n",
       "      <td>-3.601429</td>\n",
       "      <td>3.768399</td>\n",
       "      <td>AER</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.880208</td>\n",
       "      <td>-2.233010</td>\n",
       "      <td>3.993427</td>\n",
       "      <td>2.252509</td>\n",
       "      <td>-5.714414</td>\n",
       "      <td>10.219431</td>\n",
       "      <td>ALL</td>\n",
       "      <td>jra55</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.589753</td>\n",
       "      <td>0.049677</td>\n",
       "      <td>1.129829</td>\n",
       "      <td>1.826655</td>\n",
       "      <td>0.153867</td>\n",
       "      <td>3.499444</td>\n",
       "      <td>GHG</td>\n",
       "      <td>jra55</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.977667</td>\n",
       "      <td>-1.900404</td>\n",
       "      <td>5.855738</td>\n",
       "      <td>0.726001</td>\n",
       "      <td>-0.697638</td>\n",
       "      <td>2.149641</td>\n",
       "      <td>NAT</td>\n",
       "      <td>jra55</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.873586</td>\n",
       "      <td>-5.530290</td>\n",
       "      <td>3.783118</td>\n",
       "      <td>-0.043988</td>\n",
       "      <td>-0.278467</td>\n",
       "      <td>0.190492</td>\n",
       "      <td>AER</td>\n",
       "      <td>jra55</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.978482</td>\n",
       "      <td>-0.320706</td>\n",
       "      <td>2.277671</td>\n",
       "      <td>2.508669</td>\n",
       "      <td>-0.822239</td>\n",
       "      <td>5.839576</td>\n",
       "      <td>ALL</td>\n",
       "      <td>jra55</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.605286</td>\n",
       "      <td>0.010936</td>\n",
       "      <td>1.199635</td>\n",
       "      <td>1.864482</td>\n",
       "      <td>0.033687</td>\n",
       "      <td>3.695277</td>\n",
       "      <td>GHG</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.247730</td>\n",
       "      <td>-3.237553</td>\n",
       "      <td>3.733012</td>\n",
       "      <td>0.055870</td>\n",
       "      <td>-0.730160</td>\n",
       "      <td>0.841900</td>\n",
       "      <td>NAT</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.079108</td>\n",
       "      <td>-0.442400</td>\n",
       "      <td>4.600615</td>\n",
       "      <td>1.527919</td>\n",
       "      <td>-0.325116</td>\n",
       "      <td>3.380953</td>\n",
       "      <td>AER</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.982189</td>\n",
       "      <td>-0.290984</td>\n",
       "      <td>2.255363</td>\n",
       "      <td>3.448270</td>\n",
       "      <td>-1.021588</td>\n",
       "      <td>7.918129</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.540973</td>\n",
       "      <td>-0.537361</td>\n",
       "      <td>1.619307</td>\n",
       "      <td>1.689497</td>\n",
       "      <td>-1.678218</td>\n",
       "      <td>5.057212</td>\n",
       "      <td>GHG</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.555822</td>\n",
       "      <td>-2.005216</td>\n",
       "      <td>3.116861</td>\n",
       "      <td>0.198429</td>\n",
       "      <td>-0.715864</td>\n",
       "      <td>1.112723</td>\n",
       "      <td>NAT</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.436035</td>\n",
       "      <td>-5.220353</td>\n",
       "      <td>4.348282</td>\n",
       "      <td>-0.335838</td>\n",
       "      <td>-4.020752</td>\n",
       "      <td>3.349077</td>\n",
       "      <td>AER</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.817742</td>\n",
       "      <td>-3.379755</td>\n",
       "      <td>5.015240</td>\n",
       "      <td>1.552089</td>\n",
       "      <td>-6.414834</td>\n",
       "      <td>9.519012</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>EAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.582899</td>\n",
       "      <td>0.042824</td>\n",
       "      <td>1.122975</td>\n",
       "      <td>1.805427</td>\n",
       "      <td>0.132639</td>\n",
       "      <td>3.478215</td>\n",
       "      <td>GHG</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.601484</td>\n",
       "      <td>-3.276587</td>\n",
       "      <td>4.479555</td>\n",
       "      <td>0.220805</td>\n",
       "      <td>-1.202835</td>\n",
       "      <td>1.644444</td>\n",
       "      <td>NAT</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.077841</td>\n",
       "      <td>-3.578863</td>\n",
       "      <td>5.734545</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>-0.180207</td>\n",
       "      <td>0.288752</td>\n",
       "      <td>AER</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.993324</td>\n",
       "      <td>-0.596997</td>\n",
       "      <td>2.583646</td>\n",
       "      <td>2.080504</td>\n",
       "      <td>-1.250403</td>\n",
       "      <td>5.411412</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ncep2</td>\n",
       "      <td>WNA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sf_best    sf_min    sf_max     trend  trend_min  trend_max forcing  \\\n",
       "0   0.628718  0.034368  1.223067  1.936660   0.105865   3.767455     GHG   \n",
       "1  -0.360311 -3.845594  3.124972 -0.081260  -0.867290   0.704769     NAT   \n",
       "2   2.127583 -0.393924  4.649090  1.563543  -0.289491   3.416577     AER   \n",
       "3   0.962847 -0.295960  2.221655  3.418943  -1.050916   7.888801     ALL   \n",
       "4   0.689954 -0.388380  1.768287  2.154775  -1.212940   5.522490     GHG   \n",
       "5   0.516582 -2.044456  3.077620  0.184420  -0.729873   1.098714     NAT   \n",
       "6   0.112446 -4.671872  4.896763  0.086606  -3.598308   3.771521     AER   \n",
       "7   0.891933 -2.037392  3.821257  2.425802  -5.541121  10.392724     ALL   \n",
       "8   0.348429 -0.191647  0.888505  1.079196  -0.593592   2.751985     GHG   \n",
       "9  -0.368774 -4.246845  3.509297 -0.135377  -1.559016   1.288263     NAT   \n",
       "10  0.953525 -3.703179  5.610228  0.048013  -0.186466   0.282492     AER   \n",
       "11  0.914144 -2.155859  3.984147  0.991833  -2.339075   4.322740     ALL   \n",
       "12  0.673497  0.079147  1.267847  2.074595   0.243800   3.905390     GHG   \n",
       "13 -0.167666 -3.652949  3.317617 -0.037813  -0.823843   0.748216     NAT   \n",
       "14  2.169803 -0.351705  4.691310  1.594569  -0.258465   3.447604     AER   \n",
       "15  0.972941 -0.224660  2.170541  3.631351  -0.838508   8.101210     ALL   \n",
       "16  0.689175 -0.389159  1.767508  2.152342  -1.215373   5.520057     GHG   \n",
       "17  0.046726 -2.514312  2.607765  0.016681  -0.897612   0.930975     NAT   \n",
       "18  0.108393 -4.675925  4.892710  0.083485  -3.601429   3.768399     AER   \n",
       "19  0.880208 -2.233010  3.993427  2.252509  -5.714414  10.219431     ALL   \n",
       "20  0.589753  0.049677  1.129829  1.826655   0.153867   3.499444     GHG   \n",
       "21  1.977667 -1.900404  5.855738  0.726001  -0.697638   2.149641     NAT   \n",
       "22 -0.873586 -5.530290  3.783118 -0.043988  -0.278467   0.190492     AER   \n",
       "23  0.978482 -0.320706  2.277671  2.508669  -0.822239   5.839576     ALL   \n",
       "24  0.605286  0.010936  1.199635  1.864482   0.033687   3.695277     GHG   \n",
       "25  0.247730 -3.237553  3.733012  0.055870  -0.730160   0.841900     NAT   \n",
       "26  2.079108 -0.442400  4.600615  1.527919  -0.325116   3.380953     AER   \n",
       "27  0.982189 -0.290984  2.255363  3.448270  -1.021588   7.918129     ALL   \n",
       "28  0.540973 -0.537361  1.619307  1.689497  -1.678218   5.057212     GHG   \n",
       "29  0.555822 -2.005216  3.116861  0.198429  -0.715864   1.112723     NAT   \n",
       "30 -0.436035 -5.220353  4.348282 -0.335838  -4.020752   3.349077     AER   \n",
       "31  0.817742 -3.379755  5.015240  1.552089  -6.414834   9.519012     ALL   \n",
       "32  0.582899  0.042824  1.122975  1.805427   0.132639   3.478215     GHG   \n",
       "33  0.601484 -3.276587  4.479555  0.220805  -1.202835   1.644444     NAT   \n",
       "34  1.077841 -3.578863  5.734545  0.054273  -0.180207   0.288752     AER   \n",
       "35  0.993324 -0.596997  2.583646  2.080504  -1.250403   5.411412     ALL   \n",
       "\n",
       "   obs_name domain  \n",
       "0      era5     EU  \n",
       "1      era5     EU  \n",
       "2      era5     EU  \n",
       "3      era5     EU  \n",
       "4      era5    EAS  \n",
       "5      era5    EAS  \n",
       "6      era5    EAS  \n",
       "7      era5    EAS  \n",
       "8      era5    WNA  \n",
       "9      era5    WNA  \n",
       "10     era5    WNA  \n",
       "11     era5    WNA  \n",
       "12    jra55     EU  \n",
       "13    jra55     EU  \n",
       "14    jra55     EU  \n",
       "15    jra55     EU  \n",
       "16    jra55    EAS  \n",
       "17    jra55    EAS  \n",
       "18    jra55    EAS  \n",
       "19    jra55    EAS  \n",
       "20    jra55    WNA  \n",
       "21    jra55    WNA  \n",
       "22    jra55    WNA  \n",
       "23    jra55    WNA  \n",
       "24    ncep2     EU  \n",
       "25    ncep2     EU  \n",
       "26    ncep2     EU  \n",
       "27    ncep2     EU  \n",
       "28    ncep2    EAS  \n",
       "29    ncep2    EAS  \n",
       "30    ncep2    EAS  \n",
       "31    ncep2    EAS  \n",
       "32    ncep2    WNA  \n",
       "33    ncep2    WNA  \n",
       "34    ncep2    WNA  \n",
       "35    ncep2    WNA  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_all.to_csv(dir + 'trends_scaling_factors_3signal_GPH.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d840bdba8f47eb0be787346cd9ab8437e248627b5cfba543f2de371244b52e2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('proenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
